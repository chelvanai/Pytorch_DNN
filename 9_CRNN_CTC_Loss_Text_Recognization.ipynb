{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725b148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d7d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35f6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq captcha_images_v2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b52a6f",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3aaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ada307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset:\n",
    "    def __init__(self, image_paths, targets, resize=None):\n",
    "        # resize = (height, width)\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        self.aug = albumentations.Compose(\n",
    "            [\n",
    "                albumentations.Normalize(\n",
    "                    mean, std, max_pixel_value=255.0, always_apply=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.image_paths[item]).convert(\"RGB\")\n",
    "        targets = self.targets[item]\n",
    "\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n",
    "            )\n",
    "\n",
    "        image = np.array(image)\n",
    "        augmented = self.aug(image=image)\n",
    "        image = augmented[\"image\"]\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"images\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13147e1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdf957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptchaModel(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(CaptchaModel, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(3, 128, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv_2 = nn.Conv2d(128, 64, kernel_size=(3, 6), padding=(1, 1))\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.linear_1 = nn.Linear(1152, 64)\n",
    "        self.drop_1 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True)\n",
    "        self.output = nn.Linear(64, num_chars + 1)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        bs, _, _, _ = images.size()\n",
    "        x = F.relu(self.conv_1(images))\n",
    "        x = self.pool_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool_2(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.view(bs, x.size(1), -1)\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.drop_1(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.output(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        if targets is not None:\n",
    "            log_probs = F.log_softmax(x, 2)\n",
    "            input_lengths = torch.full(\n",
    "                size=(bs,), fill_value=log_probs.size(0), dtype=torch.int32\n",
    "            )\n",
    "            target_lengths = torch.full(\n",
    "                size=(bs,), fill_value=targets.size(1), dtype=torch.int32\n",
    "            )\n",
    "            loss = nn.CTCLoss(blank=0)(\n",
    "                log_probs, targets, input_lengths, target_lengths\n",
    "            )\n",
    "            return x, loss\n",
    "\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaabfd5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f0a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"captcha_images_v2\"\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_WIDTH = 100\n",
    "IMAGE_HEIGHT = 75\n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 10\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    fin_loss = 0\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    for data in tk0:\n",
    "        for key, value in data.items():\n",
    "            data[key] = value.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        fin_loss += loss.item()\n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(model, data_loader):\n",
    "    model.eval()\n",
    "    fin_loss = 0\n",
    "    fin_preds = []\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    for data in tk0:\n",
    "        for key, value in data.items():\n",
    "            data[key] = value.to(DEVICE)\n",
    "        batch_preds, loss = model(**data)\n",
    "        fin_loss += loss.item()\n",
    "        fin_preds.append(batch_preds)\n",
    "    return fin_preds, fin_loss / len(data_loader)\n",
    "\n",
    "\n",
    "\n",
    "def remove_duplicates(x):\n",
    "    if len(x) < 2:\n",
    "        return x\n",
    "    fin = \"\"\n",
    "    for j in x:\n",
    "        if fin == \"\":\n",
    "            fin = j\n",
    "        else:\n",
    "            if j == fin[-1]:\n",
    "                continue\n",
    "            else:\n",
    "                fin = fin + j\n",
    "    return fin\n",
    "\n",
    "\n",
    "def decode_predictions(preds, encoder):\n",
    "    preds = preds.permute(1, 0, 2)\n",
    "    preds = torch.softmax(preds, 2)\n",
    "    preds = torch.argmax(preds, 2)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    cap_preds = []\n",
    "    for j in range(preds.shape[0]):\n",
    "        temp = []\n",
    "        for k in preds[j, :]:\n",
    "            k = k - 1\n",
    "            if k == -1:\n",
    "                temp.append(\"§\")\n",
    "            else:\n",
    "                p = encoder.inverse_transform([k])[0]\n",
    "                temp.append(p)\n",
    "        tp = \"\".join(temp).replace(\"§\", \"\")\n",
    "        cap_preds.append(remove_duplicates(tp))\n",
    "    return cap_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c1df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    image_files = glob.glob(os.path.join(DATA_DIR, \"*.png\"))\n",
    "    targets_orig = [x.split(\"/\")[-1][:-4] for x in image_files]\n",
    "    targets = [[c for c in x] for x in targets_orig]\n",
    "    targets_flat = [c for clist in targets for c in clist]\n",
    "\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    lbl_enc.fit(targets_flat)\n",
    "    targets_enc = [lbl_enc.transform(x) for x in targets]\n",
    "    targets_enc = np.array(targets_enc)\n",
    "    targets_enc = targets_enc + 1\n",
    "\n",
    "    (\n",
    "        train_imgs,\n",
    "        test_imgs,\n",
    "        train_targets,\n",
    "        test_targets,\n",
    "        _,\n",
    "        test_targets_orig,\n",
    "    ) = model_selection.train_test_split(\n",
    "        image_files, targets_enc, targets_orig, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = ClassificationDataset(\n",
    "        image_paths=train_imgs,\n",
    "        targets=train_targets,\n",
    "        resize=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_dataset = ClassificationDataset(\n",
    "        image_paths=test_imgs,\n",
    "        targets=test_targets,\n",
    "        resize=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    model = CaptchaModel(num_chars=len(lbl_enc.classes_))\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.8, patience=5, verbose=True\n",
    "    )\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(model, train_loader, optimizer)\n",
    "        valid_preds, test_loss = eval_fn(model, test_loader)\n",
    "        valid_captcha_preds = []\n",
    "        for vp in valid_preds:\n",
    "            current_preds = decode_predictions(vp, lbl_enc)\n",
    "            valid_captcha_preds.extend(current_preds)\n",
    "        combined = list(zip(test_targets_orig, valid_captcha_preds))\n",
    "        print(combined[:10])\n",
    "        test_dup_rem = [remove_duplicates(c) for c in test_targets_orig]\n",
    "        accuracy = metrics.accuracy_score(test_dup_rem, valid_captcha_preds)\n",
    "        print(\n",
    "            f\"Epoch={epoch}, Train Loss={train_loss}, Test Loss={test_loss} Accuracy={accuracy}\"\n",
    "        )\n",
    "        scheduler.step(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded5a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 122.23it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 85.87it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', ''), ('43xfe', ''), ('x347n', ''), ('33f7m', ''), ('mn5c4', ''), ('c6f8g', ''), ('77387', ''), ('3xcgg', ''), ('25w53', ''), ('gd8fb', '')]\n",
      "Epoch=0, Train Loss=3.4861614795831533, Test Loss=3.2792163628798265 Accuracy=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 119.52it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 78.12it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', ''), ('43xfe', ''), ('x347n', ''), ('33f7m', ''), ('mn5c4', ''), ('c6f8g', ''), ('77387', ''), ('3xcgg', ''), ('25w53', ''), ('gd8fb', '')]\n",
      "Epoch=1, Train Loss=3.2418808101588845, Test Loss=3.21640598315459 Accuracy=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 119.04it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 88.02it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', ''), ('43xfe', ''), ('x347n', ''), ('33f7m', ''), ('mn5c4', ''), ('c6f8g', ''), ('77387', ''), ('3xcgg', ''), ('25w53', ''), ('gd8fb', '')]\n",
      "Epoch=2, Train Loss=3.057123135297726, Test Loss=2.6681850048211904 Accuracy=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 121.57it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 84.08it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', ''), ('43xfe', '43f'), ('x347n', '347'), ('33f7m', '3f'), ('mn5c4', '54'), ('c6f8g', 'f8g'), ('77387', '587'), ('3xcgg', '5g'), ('25w53', '25'), ('gd8fb', 'g8f')]\n",
      "Epoch=3, Train Loss=2.241186129486459, Test Loss=1.5904359783117588 Accuracy=0.009615384615384616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 120.95it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 84.37it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', 'gen'), ('43xfe', '43fe'), ('x347n', 'n347n'), ('33f7m', '3f7n'), ('mn5c4', 'n5c4'), ('c6f8g', 'cf8g'), ('77387', '7587'), ('3xcgg', '3ncg'), ('25w53', '25n53'), ('gd8fb', 'gd8fb')]\n",
      "Epoch=4, Train Loss=1.227692259491509, Test Loss=0.7275623197738941 Accuracy=0.28846153846153844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 123.89it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 97.28it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', 'ygen'), ('43xfe', '43xfe'), ('x347n', 'x347n'), ('33f7m', '3f7n'), ('mn5c4', 'n5c4'), ('c6f8g', 'cf8g'), ('77387', '7387'), ('3xcgg', '5xcg'), ('25w53', '25w53'), ('gd8fb', 'gd8fb')]\n",
      "Epoch=5, Train Loss=0.7195539942854999, Test Loss=0.5107996243123825 Accuracy=0.5096153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 130.23it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 91.28it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', 'ygen'), ('43xfe', '43xfe'), ('x347n', 'x347n'), ('33f7m', '3f7m'), ('mn5c4', 'n5c4'), ('c6f8g', 'e6f8g'), ('77387', '7387'), ('3xcgg', '3xcg'), ('25w53', '25w53'), ('gd8fb', 'gd8fb')]\n",
      "Epoch=6, Train Loss=0.48422338390070147, Test Loss=0.2518737767464839 Accuracy=0.7596153846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 128.64it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 94.32it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', 'ygen'), ('43xfe', '43xfe'), ('x347n', 'x347n'), ('33f7m', '3f7m'), ('mn5c4', 'n5c4'), ('c6f8g', 'c6f8g'), ('77387', '7387'), ('3xcgg', '3xcg'), ('25w53', '25w53'), ('gd8fb', 'gd8fb')]\n",
      "Epoch=7, Train Loss=0.31348752652286976, Test Loss=0.17322492019201702 Accuracy=0.7692307692307693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 127.78it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 91.63it/s]\n",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', 'ygen'), ('43xfe', '43xfe'), ('x347n', 'x347n'), ('33f7m', '3f7m'), ('mn5c4', 'mn5c4'), ('c6f8g', 'c6f8g'), ('77387', '7387'), ('3xcgg', '3xcg'), ('25w53', '25w53'), ('gd8fb', 'gd8fb')]\n",
      "Epoch=8, Train Loss=0.25010242597319376, Test Loss=0.13368436701309222 Accuracy=0.8173076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:03<00:00, 132.23it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 94.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ygenn', 'ygen'), ('43xfe', '43xfe'), ('x347n', 'x347n'), ('33f7m', '3f7m'), ('mn5c4', 'mn5c4'), ('c6f8g', 'c6f8g'), ('77387', '7387'), ('3xcgg', '3xcg'), ('25w53', '25w53'), ('gd8fb', 'gd8fb')]\n",
      "Epoch=9, Train Loss=0.1863732427979509, Test Loss=0.09827832032281619 Accuracy=0.9038461538461539\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8fe1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

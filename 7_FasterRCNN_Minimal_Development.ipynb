{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6f45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from __future__ import print_function, division\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf9fee",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(anchor, bbox):\n",
    "    (x1, y1, x2, y2) = anchor\n",
    "    (x3, y3, x4, y4) = bbox\n",
    "\n",
    "    intersect_width = max(0.0, min(x2, x4) - max(x1, x3))\n",
    "    intersect_height = max(0.0, min(y2, y4) - max(y1, y3))\n",
    "    intersect = intersect_width * intersect_height\n",
    "    return intersect / ((y2 - y1) * (x2 - x1) + (y4 - y3) * (x4 - x3) - intersect)\n",
    "\n",
    "def parse_pbtxt(file):\n",
    "    lines = open(file, 'r+').readlines()\n",
    "    text = ''.join(lines)\n",
    "    items = re.findall(\"item {([^}]*)}\", text)\n",
    "    return [dict(re.findall(\"(\\w*): '*([^\\n']*)'*\", item)) for item in items]\n",
    "\n",
    "def get_label_map_from_pbtxt(file):\n",
    "    items = parse_pbtxt(file)\n",
    "    result = {}\n",
    "    for item in items:\n",
    "        result[int(item['id'])] = item['name']\n",
    "    return result\n",
    "\n",
    "def get_inverse_label_map_from_pbtxt(file):\n",
    "    items = parse_pbtxt(file)\n",
    "    result = {}\n",
    "    for item in items:\n",
    "        result[item['name']] = int(item['id'])\n",
    "    return result\n",
    "\n",
    "def nms(dets, cls, thresh):\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = cls\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order.item(0)\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "\n",
    "def parametrize(anchors, bboxes):\n",
    "    reg = np.zeros(anchors.shape, dtype=np.float32)\n",
    "    if not len(bboxes):\n",
    "        return reg\n",
    "\n",
    "    reg[:, 0] = 0.5 * (bboxes[:, 0] + bboxes[:, 2] - anchors[:, 0] - anchors[:, 2]) / (anchors[:, 2] - anchors[:, 0])\n",
    "    reg[:, 1] = 0.5 * (bboxes[:, 1] + bboxes[:, 3] - anchors[:, 1] - anchors[:, 3]) / (anchors[:, 3] - anchors[:, 1])\n",
    "    reg[:, 2] = np.log((bboxes[:, 2] - bboxes[:, 0]) / (anchors[:, 2] - anchors[:, 0]) )\n",
    "    reg[:, 3] = np.log((bboxes[:, 3] - bboxes[:, 1]) / (anchors[:, 3] - anchors[:, 1]) )\n",
    "    # print(reg)\n",
    "    return reg\n",
    "\n",
    "def unparametrize(anchors, reg):\n",
    "    reg = reg.view(anchors.shape).float()\n",
    "    bboxes = torch.zeros(anchors.shape, dtype=torch.float64)\n",
    "\n",
    "    bboxes[:, 0] = (anchors[:, 2] - anchors[:, 0]) * reg[:, 0] + (anchors[:, 0] + anchors[:, 2]) / 2.0\n",
    "    bboxes[:, 1] = (anchors[:, 3] - anchors[:, 1]) * reg[:, 1] + (anchors[:, 1] + anchors[:, 3]) / 2.0\n",
    "    bboxes[:, 2] = (anchors[:, 2] - anchors[:, 0]) * torch.exp(reg[:, 2])\n",
    "    bboxes[:, 3] = (anchors[:, 3] - anchors[:, 1]) * torch.exp(reg[:, 3])\n",
    "\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2.0\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2.0\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "\n",
    "    return bboxes.float()\n",
    "\n",
    "def count_positive_anchors_on_image(i, dataset):\n",
    "    bboxes = dataset.get_truth_bboxes(i)\n",
    "    anchors, _ = dataset.get_image_anchors()\n",
    "    truth_bbox, positives, negatives = dataset.get_positive_negative_anchors(anchors, bboxes)\n",
    "    print(anchors[np.where(positives)])\n",
    "    return len(np.where(positives))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89f52f",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e3cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    INPUT_SIZE = (1600, 800)\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images under VOC format.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.label_map_path = os.path.join(root_dir, 'pascal_label_map.pbtxt')\n",
    "        self.tooth_images_paths = os.listdir(os.path.join(root_dir, 'Annotations'))\n",
    "        self.label_map = self.get_label_map(self.label_map_path)\n",
    "        self.inverse_label_map = self.get_inverse_label_map(self.label_map_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tooth_images_paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = self.get_image(self.tooth_images_paths[i].split(\".\")[0])\n",
    "        bboxes, classes = self.get_truth_bboxes(self.tooth_images_paths[i].split(\".\")[0])\n",
    "        # image input is grayscale, convert to rgb\n",
    "        im = np.expand_dims(np.stack((resize(image, self.INPUT_SIZE),) * 3), axis=0)\n",
    "        return im, bboxes, classes\n",
    "\n",
    "    def get_classes(self):\n",
    "        return list(self.inverse_label_map.values())\n",
    "\n",
    "    def get_image(self, i):\n",
    "        path = os.path.join(self.root_dir, 'JPEGImages', str(i) + '.jpg')\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        return self.preprocess_image(img)\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n",
    "        cl = clahe.apply(img)\n",
    "        return cl\n",
    "\n",
    "    def get_truth_bboxes(self, i):\n",
    "        path = os.path.join(self.root_dir, 'Annotations', str(i) + '.xml')\n",
    "        tree = ET.parse(path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # we need to resize the bboxes to the INPUT_SIZE\n",
    "        size = root.find('size')\n",
    "        height = int(size.find('height').text)\n",
    "        width = int(size.find('width').text)\n",
    "        width_ratio = float(width) / float(self.INPUT_SIZE[0])\n",
    "        height_ratio = float(height) / float(self.INPUT_SIZE[1])\n",
    "\n",
    "        raw_boxes = [child for child in root if child.tag == 'object']\n",
    "        bboxes = np.array([[[int(d.text) for d in c] for c in object if c.tag == 'bndbox'] for object in raw_boxes])\n",
    "        classes = np.array(\n",
    "            [int(self.inverse_label_map[c.text]) for object in raw_boxes for c in object if c.tag == 'name'])\n",
    "        if not len(bboxes):\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        bboxes = bboxes.reshape(-1, bboxes.shape[-1])\n",
    "        for i in [0, 2]:\n",
    "            bboxes[:, i] = bboxes[:, i] / width_ratio\n",
    "        for i in [1, 3]:\n",
    "            bboxes[:, i] = bboxes[:, i] / height_ratio\n",
    "        return bboxes, classes\n",
    "\n",
    "    def get_label_map(self, label_map_path):\n",
    "        return get_label_map_from_pbtxt(label_map_path)\n",
    "\n",
    "    def get_inverse_label_map(self, label_map_path):\n",
    "        return get_inverse_label_map_from_pbtxt(label_map_path)\n",
    "\n",
    "    def get_resized_image(self, i):\n",
    "        image = self.get_image(i)\n",
    "        temp_im = Image.fromarray(image).resize(self.INPUT_SIZE)\n",
    "        im = Image.new('RGB', temp_im.size)\n",
    "        im.paste(temp_im)\n",
    "        return im\n",
    "\n",
    "    def visualise_proposals_on_image(self, bboxes, i):\n",
    "        im = self.get_resized_image(i)\n",
    "        draw = ImageDraw.Draw(im)\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            draw.rectangle([bbox[0], bbox[1], bbox[2], bbox[3]], outline='blue')\n",
    "\n",
    "        im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21418e5",
   "metadata": {},
   "source": [
    "### RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52393ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    INPUT_SIZE = (1600, 800)\n",
    "    OUTPUT_SIZE = (100, 50)\n",
    "    OUTPUT_CELL_SIZE = float(INPUT_SIZE[0]) / float(OUTPUT_SIZE[0])\n",
    "\n",
    "    # anchors constants\n",
    "    ANCHORS_RATIOS = [0.25, 0.5, 0.9]\n",
    "    ANCHORS_SCALES = [4, 6, 8]\n",
    "\n",
    "    NUMBER_ANCHORS_WIDE = OUTPUT_SIZE[0]\n",
    "    NUMBER_ANCHORS_HEIGHT = OUTPUT_SIZE[1]\n",
    "\n",
    "    NEGATIVE_THRESHOLD = 0.3\n",
    "    POSITIVE_THRESHOLD = 0.6\n",
    "\n",
    "    ANCHOR_SAMPLING_SIZE = 256\n",
    "\n",
    "    NMS_THRESHOLD = 0.5\n",
    "    PRE_NMS_MAX_PROPOSALS = 6000\n",
    "    POST_NMS_MAX_PROPOSALS = 100\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super(RPN, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.anchor_dimensions = self.get_anchor_dimensions()\n",
    "        self.anchor_number = len(self.anchor_dimensions)\n",
    "        mid_layers = 1024\n",
    "        self.RPN_conv = nn.Conv2d(self.in_dim, mid_layers, 3, 1, 1)\n",
    "        # cls layer\n",
    "        self.cls_layer = nn.Conv2d(mid_layers, 2  * self.anchor_number, 1, 1, 0)\n",
    "        # reg_layer\n",
    "        self.reg_layer = nn.Conv2d(mid_layers, 4 * self.anchor_number, 1, 1, 0)\n",
    "\n",
    "        #initialize layers\n",
    "        torch.nn.init.normal_(self.RPN_conv.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.reg_layer.weight, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Takes feature map as input'''\n",
    "        rpn_conv = F.relu(self.RPN_conv(x), inplace=True)\n",
    "        # permute dimensions\n",
    "        cls_output = self.cls_layer(rpn_conv).permute(0, 2, 3, 1).contiguous().view(1, -1, 2)\n",
    "        reg_output = self.reg_layer(rpn_conv).permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "\n",
    "        cls_output = F.softmax(cls_output.view(-1, 2), dim=1)\n",
    "        reg_output = reg_output.view(-1, 4)\n",
    "        return cls_output, reg_output\n",
    "\n",
    "    def get_target(self, bboxes):\n",
    "        anchors, filter_out = self.get_image_anchors()\n",
    "        truth_bbox, positives, negatives = self.get_positive_negative_anchors(anchors, bboxes)\n",
    "        reg_target = parametrize(anchors, truth_bbox)\n",
    "\n",
    "        n = len(anchors)\n",
    "        indices = np.array([i for i in range(n)])\n",
    "        selected_indices, positive_indices = self.get_selected_indices_sample(indices, positives, negatives)\n",
    "\n",
    "        cls_truth = np.zeros((n, 2))\n",
    "        cls_truth[np.arange(n), positives.astype(int)] = 1.0\n",
    "        return torch.from_numpy(reg_target), torch.from_numpy(cls_truth), selected_indices, positive_indices\n",
    "\n",
    "    def get_anchor_dimensions(self):\n",
    "        dimensions = []\n",
    "        for r in self.ANCHORS_RATIOS:\n",
    "            for s in self.ANCHORS_SCALES:\n",
    "                width = s * np.sqrt(r)\n",
    "                height = s * np.sqrt(1.0 / r)\n",
    "                dimensions.append((width, height))\n",
    "        return dimensions\n",
    "\n",
    "    def get_anchors_at_position(self, pos):\n",
    "        # dimensions of anchors: (self.anchor_number, 4)\n",
    "        # each anchor is [xa, ya, xb, yb]\n",
    "        x, y = pos\n",
    "        anchors = np.zeros((self.anchor_number, 4))\n",
    "        for i in range(self.anchor_number):\n",
    "            center_x = self.OUTPUT_CELL_SIZE * (float(x) + 0.5)\n",
    "            center_y = self.OUTPUT_CELL_SIZE * (float(y) + 0.5)\n",
    "\n",
    "            width = self.anchor_dimensions[i][0] * self.OUTPUT_CELL_SIZE\n",
    "            height = self.anchor_dimensions[i][1] * self.OUTPUT_CELL_SIZE\n",
    "\n",
    "            top_x = center_x - width / 2.0\n",
    "            top_y = center_y - height / 2.0\n",
    "            anchors[i, :] = [top_x, top_y, top_x + width, top_y + height]\n",
    "        return anchors\n",
    "\n",
    "    def get_proposals(self, reg, cls):\n",
    "        a, filter_out = self.get_image_anchors()\n",
    "        anchors = torch.from_numpy(a).float()\n",
    "        bboxes = unparametrize(anchors, reg).reshape((-1, 4))\n",
    "        bboxes = bboxes[filter_out]\n",
    "        objects = torch.argmax(cls[filter_out], dim=1)\n",
    "\n",
    "        cls = cls.detach().numpy()\n",
    "        cls = cls[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        bboxes = bboxes[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        keep = nms(bboxes.detach().numpy(), cls[:, 1].ravel(), self.NMS_THRESHOLD)[:self.POST_NMS_MAX_PROPOSALS]\n",
    "        return bboxes[keep]\n",
    "\n",
    "    def get_training_proposals(self, reg, cls):\n",
    "        a, filter_out = self.get_image_anchors()\n",
    "        anchors = torch.from_numpy(a).float()\n",
    "        bboxes = unparametrize(anchors, reg).reshape((-1, 4))\n",
    "        bboxes = bboxes[filter_out]\n",
    "        objects = torch.argmax(cls[filter_out], dim=1)\n",
    "\n",
    "        cls = cls.detach().numpy()\n",
    "        cls = cls[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        bboxes = bboxes[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        keep = nms(bboxes.detach().numpy(), cls[:, 1].ravel(), self.NMS_THRESHOLD)[:self.POST_NMS_MAX_PROPOSALS]\n",
    "        return bboxes[keep]\n",
    "\n",
    "    def get_image_anchors(self):\n",
    "        print('get_image_anchors')\n",
    "        anchors = np.zeros((self.NUMBER_ANCHORS_WIDE, self.NUMBER_ANCHORS_HEIGHT, self.anchor_number, 4))\n",
    "\n",
    "        for i in range(self.NUMBER_ANCHORS_WIDE):\n",
    "            for j in range(self.NUMBER_ANCHORS_HEIGHT):\n",
    "                anchors_pos = self.get_anchors_at_position((i, j))\n",
    "                anchors[i, j, :] = anchors_pos\n",
    "        anchors = anchors.reshape((-1, 4))\n",
    "        filter_out = (anchors[:, 0] < 0) | (anchors[:, 1] < 0) | (anchors[:, 2] > self.INPUT_SIZE[0]) | (anchors[:, 3] > self.INPUT_SIZE[1])\n",
    "        return anchors, np.where(~filter_out)\n",
    "\n",
    "    def get_positive_negative_anchors(self, anchors, bboxes):\n",
    "        if not len(bboxes):\n",
    "            ious = np.zeros(anchors.shape[:3])\n",
    "            positives = ious > self.POSITIVE_THRESHOLD\n",
    "            negatives = ious < self.NEGATIVE_THRESHOLD\n",
    "            return np.array([]), positives, negatives\n",
    "\n",
    "        ious = np.zeros((anchors.shape[0], len(bboxes)))\n",
    "\n",
    "        # TODO improve speed with a real numpy formula\n",
    "        for i in range(ious.shape[0]):\n",
    "            for j in range(ious.shape[1]):\n",
    "                ious[i, j] = IoU(anchors[i], bboxes[j])\n",
    "        best_bbox_for_anchor = np.argmax(ious, axis=1)\n",
    "        best_anchor_for_bbox = np.argmax(ious, axis=0)\n",
    "        max_iou_per_anchor = np.amax(ious, axis=1)\n",
    "\n",
    "        # truth box for each anchor\n",
    "        truth_bbox = bboxes[best_bbox_for_anchor, :]\n",
    "\n",
    "        # Selecting all ious > POSITIVE_THRESHOLD\n",
    "        positives = max_iou_per_anchor > self.POSITIVE_THRESHOLD\n",
    "        # Adding max iou for each ground truth box\n",
    "        positives[best_anchor_for_bbox] = True\n",
    "        negatives = max_iou_per_anchor < self.NEGATIVE_THRESHOLD\n",
    "        return truth_bbox, positives, negatives\n",
    "\n",
    "    def get_selected_indices_sample(self, indices, positives, negatives):\n",
    "        positive_indices = indices[positives]\n",
    "        negative_indices = indices[negatives]\n",
    "        random_positives = np.random.permutation(positive_indices)[:self.ANCHOR_SAMPLING_SIZE // 2]\n",
    "        random_negatives = np.random.permutation(negative_indices)[:self.ANCHOR_SAMPLING_SIZE - len(random_positives)]\n",
    "        selected_indices = np.concatenate((random_positives, random_negatives))\n",
    "        return selected_indices, positive_indices\n",
    "\n",
    "    def get_positive_anchors(self, bboxes):\n",
    "        anchors, _ = self.get_image_anchors()\n",
    "        truth_bbox, positives, negatives = self.get_positive_negative_anchors(anchors, bboxes)\n",
    "\n",
    "        n = len(anchors)\n",
    "        indices = np.array([i for i in range(n)])\n",
    "        selected_indices, positive_indices = self.get_selected_indices_sample(indices, positives, negatives)\n",
    "        return anchors[positive_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d1b02",
   "metadata": {},
   "source": [
    "### Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55122507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    INPUT_SIZE = (1600, 800)\n",
    "    OUTPUT_SIZE = (100, 50)\n",
    "    OUTPUT_CELL_SIZE = float(INPUT_SIZE[0]) / float(OUTPUT_SIZE[0])\n",
    "\n",
    "    NEGATIVE_THRESHOLD = 0.3\n",
    "    POSITIVE_THRESHOLD = 0.5\n",
    "\n",
    "    def __init__(self, n_classes, model='resnet50', path='fasterrcnn_resnet50.pt', training=False):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "\n",
    "        self.n_roi_sample = 128\n",
    "        self.pos_ratio = 0.25\n",
    "        self.pos_iou_thresh = 0.5\n",
    "        self.neg_iou_thresh_hi = 0.5\n",
    "        self.neg_iou_thresh_lo = 0.0\n",
    "\n",
    "        if model == 'resnet50':\n",
    "            self.in_dim = 1024\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "            self.feature_map = nn.Sequential(*list(resnet.children())[:-3])\n",
    "        if model == 'vgg16':\n",
    "            self.in_dim = 512\n",
    "            vgg = models.vgg16(pretrained=True)\n",
    "            self.feature_map = nn.Sequential(*list(vgg.children())[:-1])\n",
    "\n",
    "        self.n_classes = n_classes + 1\n",
    "        self.in_fc_dim = 7 * 7 * self.in_dim\n",
    "        self.out_fc_dim = 1024\n",
    "\n",
    "        rpn_path = path.replace('fasterrcnn_', '')\n",
    "        self.rpn = RPN(self.in_dim)\n",
    "        self.fc = nn.Linear(self.in_fc_dim, self.out_fc_dim)\n",
    "        self.cls_layer = nn.Linear(self.out_fc_dim, self.n_classes)\n",
    "        self.reg_layer = nn.Linear(self.out_fc_dim, self.n_classes * 4)\n",
    "\n",
    "        self.training = training\n",
    "\n",
    "        #initialize layers\n",
    "        torch.nn.init.normal_(self.fc.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.cls_layer.weight, std=0.1)\n",
    "        torch.nn.init.normal_(self.reg_layer.weight, std=0.01)\n",
    "\n",
    "        if os.path.isfile(path):\n",
    "            self.load_state_dict(torch.load(path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_map = self.feature_map(x)\n",
    "        cls, reg = self.rpn(feature_map)\n",
    "        feature_map = feature_map.view((-1, self.OUTPUT_SIZE[0], self.OUTPUT_SIZE[1]))\n",
    "        if self.training:\n",
    "            proposals = self.rpn.get_proposals(reg, cls)\n",
    "        else:\n",
    "            proposals = self.rpn.get_proposals(reg, cls)\n",
    "\n",
    "        all_cls = []\n",
    "        all_reg = []\n",
    "        for roi in proposals.int():\n",
    "            roi[np.where(roi < 0)] = 0\n",
    "            roi = roi / self.OUTPUT_CELL_SIZE\n",
    "            roi_feature_map = feature_map[:, roi[0]:roi[2]+1, roi[1]:roi[3]+1]\n",
    "            pooled_roi = F.adaptive_max_pool2d(roi_feature_map, (7, 7)).view((-1, 50176))\n",
    "            r = F.relu(self.fc(pooled_roi))\n",
    "            r_cls = self.cls_layer(r)\n",
    "            r_reg = self.reg_layer(r).view((self.n_classes, 4))\n",
    "            all_cls.append(r_cls)\n",
    "            all_reg.append(r_reg[torch.argmax(r_cls)])\n",
    "        # print(all_cls.shape, all_reg.shape)\n",
    "        return torch.stack(all_cls).view((-1, self.n_classes)), torch.stack(all_reg), proposals, cls, reg\n",
    "\n",
    "    def get_target(self, proposals, bboxes, classes):\n",
    "        ious = np.zeros((proposals.shape[0], len(bboxes)))\n",
    "        for i in range(proposals.shape[0]):\n",
    "            for j in range(len(bboxes)):\n",
    "                ious[i, j] = IoU(proposals[i], bboxes[j])\n",
    "        best_bbox_for_proposal = np.argmax(ious, axis=1)\n",
    "        best_proposal_for_bbox = np.argmax(ious, axis=0)\n",
    "        max_iou_per_proposal = np.amax(ious, axis=1)\n",
    "\n",
    "        labels = classes[best_bbox_for_proposal]\n",
    "\n",
    "        # truth box for each proposal\n",
    "        truth_bbox_for_roi = bboxes[best_bbox_for_proposal, :]\n",
    "        truth_bbox = parametrize(proposals.detach().numpy(), truth_bbox_for_roi)\n",
    "\n",
    "        # Selecting all ious > POSITIVE_THRESHOLD\n",
    "        positives = max_iou_per_proposal > self.POSITIVE_THRESHOLD\n",
    "        # TODO: improve the negatives selection\n",
    "        negatives = max_iou_per_proposal < self.POSITIVE_THRESHOLD\n",
    "        # Assign 'other' label to negatives\n",
    "        labels[negatives] = 0\n",
    "\n",
    "        # Keep positives and negatives\n",
    "        selected = np.where(positives | negatives)\n",
    "\n",
    "        return torch.from_numpy(labels[selected]), torch.from_numpy(truth_bbox[selected])\n",
    "\n",
    "    def get_proposals(self, reg, cls, rpn_proposals):\n",
    "        # print(cls)\n",
    "        # print(F.softmax(cls, dim=1))\n",
    "        # print(cls.shape)\n",
    "        objects = torch.argmax(F.softmax(cls, dim=1), dim=1)\n",
    "        bboxes = unparametrize(rpn_proposals, reg)\n",
    "\n",
    "        return bboxes[np.where(objects != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb880b04",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e23d0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'resnet50'\n",
    "MODEL_PATH = os.path.join('models', f'fasterrcnn_{model}.pt')\n",
    "\n",
    "def train(dataset):\n",
    "    save_range = 40\n",
    "    lamb = 10.0\n",
    "    n_classes = len(dataset.get_classes())\n",
    "\n",
    "    fasterrcnn = FasterRCNN(n_classes, model=model, path=MODEL_PATH, training=True)\n",
    "    optimizer = optim.Adam(fasterrcnn.parameters(), lr = 0.0001)\n",
    "\n",
    "    for i in range(1, len(dataset)):\n",
    "        im, bboxes, classes = dataset[i]\n",
    "        print(im.shape)\n",
    "        if not len(classes):\n",
    "            continue\n",
    "        print(i)\n",
    "        optimizer.zero_grad()\n",
    "        all_cls, all_reg, proposals, rpn_cls, rpn_reg = fasterrcnn(torch.from_numpy(im).float())\n",
    "\n",
    "        rpn_reg_target, rpn_cls_target, rpn_selected_indices, rpn_positives = fasterrcnn.rpn.get_target(bboxes)\n",
    "        cls_target, reg_target = fasterrcnn.get_target(proposals, bboxes, classes)\n",
    "        print(cls_target)\n",
    "\n",
    "        rpn_reg_loss = F.smooth_l1_loss(rpn_reg[rpn_positives], rpn_reg_target[rpn_positives])\n",
    "        # look at a sample of positive + negative boxes for classification\n",
    "        rpn_cls_loss = F.binary_cross_entropy(rpn_cls[rpn_selected_indices], rpn_cls_target[rpn_selected_indices].float())\n",
    "\n",
    "        fastrcnn_reg_loss = F.smooth_l1_loss(all_reg, reg_target)\n",
    "        fastrcnn_cls_loss = F.cross_entropy(all_cls, cls_target)\n",
    "        rpn_loss = rpn_cls_loss + lamb * rpn_reg_loss\n",
    "\n",
    "        fastrcnn_loss = fastrcnn_cls_loss + fastrcnn_reg_loss\n",
    "        print(rpn_reg_loss, rpn_cls_loss, fastrcnn_reg_loss, fastrcnn_cls_loss)\n",
    "        loss = rpn_loss + fastrcnn_loss\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('[%d] loss: %.5f'.format(i, loss.item()))\n",
    "\n",
    "        if i % save_range == 0:\n",
    "            torch.save(fasterrcnn.state_dict(), MODEL_PATH)\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4055ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VOCDataset('VOC2007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53c246a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VOCDataset at 0x7fb3af303490>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd69481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_Seq2Seq_Audio_Style_Change_On_Tactron2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0TepGVw2PV9"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install torch==1.6\n",
        "!pip install torch-stft\n",
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_3Fqi_4cTz"
      },
      "source": [
        "### Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9yojjmz4etm",
        "outputId": "5bc01d6a-181a-4df3-ebc8-8ee5c60b8bbe"
      },
      "source": [
        "!gdown --id 1tac1L8Ccbl5_Vo1IbkMww3fBEjJ7VVaX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tac1L8Ccbl5_Vo1IbkMww3fBEjJ7VVaX\n",
            "To: /content/mellotron_Datasets.zip\n",
            "105MB [00:02, 37.4MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2schQS34pUC"
      },
      "source": [
        "!unzip mellotron_Datasets.zip\n",
        "!mv mellotron_Datasets/Blizzard_train_filelist.txt ./\n",
        "!mv mellotron_Datasets/Blizzard_val_filelist.txt ./\n",
        "!mv mellotron_Datasets/Blizzard_Emotion_Data ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDcZbSYn5zMO",
        "outputId": "38a3bd24-5eeb-4078-e74d-75c2d1c4a0bf"
      },
      "source": [
        "!gdown --id 1uBuZZpuDHhqj4pE5rM67GEgBidam56Fn\n",
        "!unzip text.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uBuZZpuDHhqj4pE5rM67GEgBidam56Fn\n",
            "To: /content/text.zip\n",
            "\r  0% 0.00/13.8k [00:00<?, ?B/s]\r100% 13.8k/13.8k [00:00<00:00, 6.70MB/s]\n",
            "Archive:  text.zip\n",
            "   creating: text/\n",
            "  inflating: text/LICENSE            \n",
            "  inflating: text/__init__.py        \n",
            "  inflating: text/cleaners.py        \n",
            "  inflating: text/cmudict.py         \n",
            "  inflating: text/numbers.py         \n",
            "  inflating: text/symbols.py         \n",
            "   creating: text/__pycache__/\n",
            "  inflating: text/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: text/__pycache__/cleaners.cpython-37.pyc  \n",
            "  inflating: text/__pycache__/numbers.cpython-37.pyc  \n",
            "  inflating: text/__pycache__/symbols.cpython-37.pyc  \n",
            "  inflating: text/__pycache__/cmudict.cpython-37.pyc  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq2W37H_9FhE",
        "outputId": "36caa12b-8e57-4b0e-d245-727531eb6d99"
      },
      "source": [
        "!gdown --id 1FW1qIYI1cvhmIn6hpBl0qSq8HlKEC8G-\n",
        "!unzip -j fgg.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FW1qIYI1cvhmIn6hpBl0qSq8HlKEC8G-\n",
            "To: /content/fgg.zip\n",
            "\r  0% 0.00/5.75k [00:00<?, ?B/s]\r100% 5.75k/5.75k [00:00<00:00, 8.12MB/s]\n",
            "Archive:  fgg.zip\n",
            "  inflating: audio_processing.py     \n",
            "  inflating: hparams.py              \n",
            "  inflating: layers.py               \n",
            "  inflating: yin.py                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZREjXTsfHYC2",
        "outputId": "d8bd4d26-e61d-4a58-87b7-6978168ab55a"
      },
      "source": [
        "!gdown --id 1VSV4TapY7c-3-goRcZD_jLwNf7toFXm9\n",
        "!unzip data.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VSV4TapY7c-3-goRcZD_jLwNf7toFXm9\n",
            "To: /content/data.zip\n",
            "\r  0% 0.00/1.62M [00:00<?, ?B/s]\r100% 1.62M/1.62M [00:00<00:00, 108MB/s]\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/cmu_dictionary     \n",
            "  inflating: data/debussy_prelude_lyrics.musicxml  \n",
            "  inflating: data/example1.wav       \n",
            "  inflating: data/example2.wav       \n",
            "  inflating: data/haendel_hallelujah.musicxml  \n",
            "  inflating: data/mozart_requiem_kyrie_satb.musicxml  \n",
            "  inflating: data/example3.wav       \n",
            "  inflating: data/Untitled.ipynb     \n",
            "  inflating: data/examples_filelist.txt  \n",
            "   creating: data/.ipynb_checkpoints/\n",
            "  inflating: data/.ipynb_checkpoints/Untitled-checkpoint.ipynb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaZ1T4wDAE6J"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from math import sqrt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from scipy.io.wavfile import read\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "import layers\n",
        "from hparams import create_hparams\n",
        "from layers import ConvNorm, LinearNorm\n",
        "from text import text_to_sequence,cmudict\n",
        "from yin import compute_yin"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMzX2QsmDiZx"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8vL1G_1Djc_"
      },
      "source": [
        "def get_mask_from_lengths(lengths):\n",
        "    max_len = torch.max(lengths).item()\n",
        "    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
        "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_wav_to_torch(full_path):\n",
        "    sampling_rate, data = read(full_path)\n",
        "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
        "\n",
        "\n",
        "def load_filepaths_and_text(filename, split=\"|\"):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
        "    return filepaths_and_text\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    x = x.contiguous()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda(non_blocking=True)\n",
        "    return torch.autograd.Variable(x)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NqWw9xgDp3x"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kANqbV_YDq60"
      },
      "source": [
        "class TextMelLoader(torch.utils.data.Dataset):\n",
        "    def __init__(self, audiopaths_and_text, hparams, speaker_ids=None):\n",
        "        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n",
        "        self.text_cleaners = hparams.text_cleaners\n",
        "        self.max_wav_value = hparams.max_wav_value\n",
        "        self.sampling_rate = hparams.sampling_rate\n",
        "        self.stft = layers.TacotronSTFT(\n",
        "            hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "            hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "            hparams.mel_fmax)\n",
        "        self.sampling_rate = hparams.sampling_rate\n",
        "        self.filter_length = hparams.filter_length\n",
        "        self.hop_length = hparams.hop_length\n",
        "        self.f0_min = hparams.f0_min\n",
        "        self.f0_max = hparams.f0_max\n",
        "        self.harm_thresh = hparams.harm_thresh\n",
        "        self.p_arpabet = hparams.p_arpabet\n",
        "\n",
        "        self.cmudict = None\n",
        "        if hparams.cmudict_path is not None:\n",
        "            self.cmudict = cmudict.CMUDict(hparams.cmudict_path)\n",
        "\n",
        "        self.speaker_ids = speaker_ids\n",
        "        if speaker_ids is None:\n",
        "            self.speaker_ids = self.create_speaker_lookup_table(\n",
        "                self.audiopaths_and_text)\n",
        "\n",
        "        random.seed(1234)\n",
        "        random.shuffle(self.audiopaths_and_text)\n",
        "\n",
        "    def create_speaker_lookup_table(self, audiopaths_and_text):\n",
        "        speaker_ids = np.sort(np.unique([x[2] for x in audiopaths_and_text]))\n",
        "        print('speaker id ', speaker_ids)\n",
        "        d = {int(speaker_ids[i]): i for i in range(len(speaker_ids))}\n",
        "        print(\"seected speaker id \", d)\n",
        "        return d\n",
        "\n",
        "    def get_f0(self, audio, sampling_rate=22050, frame_length=1024,\n",
        "               hop_length=256, f0_min=100, f0_max=300, harm_thresh=0.1):\n",
        "        f0, harmonic_rates, argmins, times = compute_yin(\n",
        "            audio, sampling_rate, frame_length, hop_length, f0_min, f0_max,\n",
        "            harm_thresh)\n",
        "        pad = int((frame_length / hop_length) / 2)\n",
        "        f0 = [0.0] * pad + f0 + [0.0] * pad\n",
        "\n",
        "        f0 = np.array(f0, dtype=np.float32)\n",
        "        return f0\n",
        "\n",
        "    def get_data(self, audiopath_and_text):\n",
        "        audiopath, text, speaker = audiopath_and_text\n",
        "        text = self.get_text(text)\n",
        "        mel, f0 = self.get_mel_and_f0(audiopath)\n",
        "        speaker_id = self.get_speaker_id(speaker)\n",
        "        return (text, mel, speaker_id, f0)\n",
        "\n",
        "    def get_speaker_id(self, speaker_id):\n",
        "        return torch.IntTensor([self.speaker_ids[int(speaker_id)]])\n",
        "\n",
        "    def get_mel_and_f0(self, filepath):\n",
        "        audio, sampling_rate = load_wav_to_torch(filepath)\n",
        "        if sampling_rate != self.stft.sampling_rate:\n",
        "            raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                sampling_rate, self.stft.sampling_rate))\n",
        "        audio_norm = audio / self.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0)\n",
        "\n",
        "        f0 = self.get_f0(audio.cpu().numpy(), self.sampling_rate,\n",
        "                         self.filter_length, self.hop_length, self.f0_min,\n",
        "                         self.f0_max, self.harm_thresh)\n",
        "        f0 = torch.from_numpy(f0)[None]\n",
        "        f0 = f0[:, :melspec.size(1)]\n",
        "\n",
        "        return melspec, f0\n",
        "\n",
        "    def get_text(self, text):\n",
        "        text_norm = torch.IntTensor(\n",
        "            text_to_sequence(text, self.text_cleaners, self.cmudict, self.p_arpabet))\n",
        "\n",
        "        return text_norm\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_data(self.audiopaths_and_text[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audiopaths_and_text)\n",
        "\n",
        "\n",
        "class TextMelCollate():\n",
        "    def __init__(self, n_frames_per_step):\n",
        "        self.n_frames_per_step = n_frames_per_step\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # Right zero-pad all one-hot text sequences to max input length\n",
        "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
        "            torch.LongTensor([len(x[0]) for x in batch]),\n",
        "            dim=0, descending=True)\n",
        "        max_input_len = input_lengths[0]\n",
        "\n",
        "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
        "        text_padded.zero_()\n",
        "        for i in range(len(ids_sorted_decreasing)):\n",
        "            text = batch[ids_sorted_decreasing[i]][0]\n",
        "            text_padded[i, :text.size(0)] = text\n",
        "\n",
        "        # Right zero-pad mel-spec\n",
        "        num_mels = batch[0][1].size(0)\n",
        "        max_target_len = max([x[1].size(1) for x in batch])\n",
        "        if max_target_len % self.n_frames_per_step != 0:\n",
        "            max_target_len += self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
        "            assert max_target_len % self.n_frames_per_step == 0\n",
        "\n",
        "        # include mel padded, gate padded and speaker ids\n",
        "        mel_padded = torch.FloatTensor(len(batch), num_mels, max_target_len)\n",
        "        mel_padded.zero_()\n",
        "        gate_padded = torch.FloatTensor(len(batch), max_target_len)\n",
        "        gate_padded.zero_()\n",
        "        output_lengths = torch.LongTensor(len(batch))\n",
        "        speaker_ids = torch.LongTensor(len(batch))\n",
        "        f0_padded = torch.FloatTensor(len(batch), 1, max_target_len)\n",
        "        f0_padded.zero_()\n",
        "\n",
        "        for i in range(len(ids_sorted_decreasing)):\n",
        "            mel = batch[ids_sorted_decreasing[i]][1]\n",
        "            mel_padded[i, :, :mel.size(1)] = mel\n",
        "            gate_padded[i, mel.size(1) - 1:] = 1\n",
        "            output_lengths[i] = mel.size(1)\n",
        "            speaker_ids[i] = batch[ids_sorted_decreasing[i]][2]\n",
        "            f0 = batch[ids_sorted_decreasing[i]][3]\n",
        "            f0_padded[i, :, :f0.size(1)] = f0\n",
        "\n",
        "        model_inputs = (text_padded, input_lengths, mel_padded, gate_padded,\n",
        "                        output_lengths, speaker_ids, f0_padded)\n",
        "\n",
        "        return model_inputs\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK9PNInyD9k5"
      },
      "source": [
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams,\n",
        "                           speaker_ids=trainset.speaker_ids)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn, train_sampler"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn7IPUfrEQ3b",
        "outputId": "aa49608d-5e60-4f7f-f92c-1d55593c9aec"
      },
      "source": [
        "hparams = create_hparams()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODPjilf_KKGX"
      },
      "source": [
        "### Model - Tactron2-GST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJrkccuGKRTu"
      },
      "source": [
        "### Location layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BgfnEAT2Ee7"
      },
      "source": [
        "drop_rate = 0.5"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T46PNTokbk3P"
      },
      "source": [
        "class LocationLayer(nn.Module):\n",
        "    def __init__(self, attention_n_filters, attention_kernel_size,\n",
        "                 attention_dim):\n",
        "        super(LocationLayer, self).__init__()\n",
        "        padding = int((attention_kernel_size - 1) / 2)\n",
        "        self.location_conv = ConvNorm(2, attention_n_filters,\n",
        "                                      kernel_size=attention_kernel_size,\n",
        "                                      padding=padding, bias=False, stride=1,\n",
        "                                      dilation=1)\n",
        "        self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
        "                                         bias=False, w_init_gain='tanh')\n",
        "\n",
        "    def forward(self, attention_weights_cat):\n",
        "        processed_attention = self.location_conv(attention_weights_cat)\n",
        "        processed_attention = processed_attention.transpose(1, 2)\n",
        "        processed_attention = self.location_dense(processed_attention)\n",
        "        return processed_attention"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e4QQRrcKg2z"
      },
      "source": [
        "### Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBA6lQ5-br5w"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
        "                 attention_location_n_filters, attention_location_kernel_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
        "                                      bias=False, w_init_gain='tanh')\n",
        "        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
        "                                       w_init_gain='tanh')\n",
        "        self.v = LinearNorm(attention_dim, 1, bias=False)\n",
        "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
        "                                            attention_location_kernel_size,\n",
        "                                            attention_dim)\n",
        "        self.score_mask_value = -float(\"inf\")\n",
        "\n",
        "    def get_alignment_energies(self, query, processed_memory,\n",
        "                               attention_weights_cat):\n",
        "        processed_query = self.query_layer(query.unsqueeze(1))\n",
        "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
        "        energies = self.v(torch.tanh(\n",
        "            processed_query + processed_attention_weights + processed_memory))\n",
        "\n",
        "        energies = energies.squeeze(-1)\n",
        "        return energies\n",
        "\n",
        "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
        "                attention_weights_cat, mask, attention_weights=None):\n",
        "        if attention_weights is None:\n",
        "            alignment = self.get_alignment_energies(\n",
        "                attention_hidden_state, processed_memory, attention_weights_cat)\n",
        "\n",
        "            if mask is not None:\n",
        "                alignment.data.masked_fill_(mask, self.score_mask_value)\n",
        "\n",
        "            attention_weights = F.softmax(alignment, dim=1)\n",
        "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
        "        attention_context = attention_context.squeeze(1)\n",
        "\n",
        "        return attention_context, attention_weights"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL186sgHKoSV"
      },
      "source": [
        "### PreNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "975lF2ryb0zE"
      },
      "source": [
        "class Prenet(nn.Module):\n",
        "    def __init__(self, in_dim, sizes):\n",
        "        super(Prenet, self).__init__()\n",
        "        in_sizes = [in_dim] + sizes[:-1]\n",
        "        self.layers = nn.ModuleList(\n",
        "            [LinearNorm(in_size, out_size, bias=False)\n",
        "             for (in_size, out_size) in zip(in_sizes, sizes)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for linear in self.layers:\n",
        "            x = F.dropout(F.relu(linear(x)), p=drop_rate, training=True)\n",
        "        return x"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLQjElHCKurB"
      },
      "source": [
        "### PostNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvg6JUXgb4p3"
      },
      "source": [
        "class Postnet(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.convolutions = nn.ModuleList()\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.n_mel_channels, hparams.postnet_embedding_dim,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='tanh'),\n",
        "                nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "        )\n",
        "\n",
        "        for i in range(1, hparams.postnet_n_convolutions - 1):\n",
        "            self.convolutions.append(\n",
        "                nn.Sequential(\n",
        "                    ConvNorm(hparams.postnet_embedding_dim,\n",
        "                             hparams.postnet_embedding_dim,\n",
        "                             kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                             padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                             dilation=1, w_init_gain='tanh'),\n",
        "                    nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
        "            )\n",
        "\n",
        "        self.convolutions.append(\n",
        "            nn.Sequential(\n",
        "                ConvNorm(hparams.postnet_embedding_dim, hparams.n_mel_channels,\n",
        "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
        "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='linear'),\n",
        "                nn.BatchNorm1d(hparams.n_mel_channels))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.convolutions) - 1):\n",
        "            x = F.dropout(torch.tanh(self.convolutions[i](x)), drop_rate, self.training)\n",
        "        x = F.dropout(self.convolutions[-1](x), drop_rate, self.training)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH3wB0AIK0Na"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYwztHnMb8uU"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        convolutions = []\n",
        "        for _ in range(hparams.encoder_n_convolutions):\n",
        "            conv_layer = nn.Sequential(\n",
        "                ConvNorm(hparams.encoder_embedding_dim,\n",
        "                         hparams.encoder_embedding_dim,\n",
        "                         kernel_size=hparams.encoder_kernel_size, stride=1,\n",
        "                         padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
        "                         dilation=1, w_init_gain='relu'),\n",
        "                nn.BatchNorm1d(hparams.encoder_embedding_dim))\n",
        "            convolutions.append(conv_layer)\n",
        "        self.convolutions = nn.ModuleList(convolutions)\n",
        "\n",
        "        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
        "                            int(hparams.encoder_embedding_dim / 2), 1,\n",
        "                            batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x, input_lengths):\n",
        "        if x.size()[0] > 1:\n",
        "            print(\"here\")\n",
        "            x_embedded = []\n",
        "            for b_ind in range(x.size()[0]):  # TODO: Speed up\n",
        "                curr_x = x[b_ind:b_ind + 1, :, :input_lengths[b_ind]].clone()\n",
        "                for conv in self.convolutions:\n",
        "                    curr_x = F.dropout(F.relu(conv(curr_x)), drop_rate, self.training)\n",
        "                x_embedded.append(curr_x[0].transpose(0, 1))\n",
        "            x = torch.nn.utils.rnn.pad_sequence(x_embedded, batch_first=True)\n",
        "        else:\n",
        "            for conv in self.convolutions:\n",
        "                x = F.dropout(F.relu(conv(x)), drop_rate, self.training)\n",
        "            x = x.transpose(1, 2)\n",
        "\n",
        "        # pytorch tensor are not reversible, hence the conversion\n",
        "        input_lengths = input_lengths.cpu().numpy()\n",
        "        x = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, input_lengths, batch_first=True)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "            outputs, batch_first=True)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def inference(self, x):\n",
        "        for conv in self.convolutions:\n",
        "            x = F.dropout(F.relu(conv(x)), drop_rate, self.training)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        self.lstm.flatten_parameters()\n",
        "        outputs, _ = self.lstm(x)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyioSGNmMmsN"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utM9ZiZhcBEM"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        self.encoder_embedding_dim = hparams.encoder_embedding_dim + hparams.token_embedding_size + hparams.speaker_embedding_dim\n",
        "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
        "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
        "        self.prenet_dim = hparams.prenet_dim\n",
        "        self.max_decoder_steps = hparams.max_decoder_steps\n",
        "        self.gate_threshold = hparams.gate_threshold\n",
        "        self.p_attention_dropout = hparams.p_attention_dropout\n",
        "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
        "        self.p_teacher_forcing = hparams.p_teacher_forcing\n",
        "\n",
        "        self.prenet_f0 = ConvNorm(\n",
        "            1, hparams.prenet_f0_dim,\n",
        "            kernel_size=hparams.prenet_f0_kernel_size,\n",
        "            padding=max(0, int(hparams.prenet_f0_kernel_size / 2)),\n",
        "            bias=False, stride=1, dilation=1)\n",
        "\n",
        "        self.prenet = Prenet(\n",
        "            hparams.n_mel_channels * hparams.n_frames_per_step,\n",
        "            [hparams.prenet_dim, hparams.prenet_dim])\n",
        "\n",
        "        self.attention_rnn = nn.LSTMCell(\n",
        "            hparams.prenet_dim + hparams.prenet_f0_dim + self.encoder_embedding_dim,\n",
        "            hparams.attention_rnn_dim)\n",
        "\n",
        "        self.attention_layer = Attention(\n",
        "            hparams.attention_rnn_dim, self.encoder_embedding_dim,\n",
        "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
        "            hparams.attention_location_kernel_size)\n",
        "\n",
        "        self.decoder_rnn = nn.LSTMCell(\n",
        "            hparams.attention_rnn_dim + self.encoder_embedding_dim,\n",
        "            hparams.decoder_rnn_dim, 1)\n",
        "\n",
        "        self.linear_projection = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + self.encoder_embedding_dim,\n",
        "            hparams.n_mel_channels * hparams.n_frames_per_step)\n",
        "\n",
        "        self.gate_layer = LinearNorm(\n",
        "            hparams.decoder_rnn_dim + self.encoder_embedding_dim, 1,\n",
        "            bias=True, w_init_gain='sigmoid')\n",
        "\n",
        "    def get_go_frame(self, memory):\n",
        "        B = memory.size(0)\n",
        "        decoder_input = Variable(memory.data.new(\n",
        "            B, self.n_mel_channels * self.n_frames_per_step).zero_())\n",
        "        return decoder_input\n",
        "\n",
        "    def get_end_f0(self, f0s):\n",
        "        B = f0s.size(0)\n",
        "        dummy = Variable(f0s.data.new(B, 1, f0s.size(1)).zero_())\n",
        "        return dummy\n",
        "\n",
        "    def initialize_decoder_states(self, memory, mask):\n",
        "        B = memory.size(0)\n",
        "        MAX_TIME = memory.size(1)\n",
        "\n",
        "        self.attention_hidden = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "        self.attention_cell = Variable(memory.data.new(\n",
        "            B, self.attention_rnn_dim).zero_())\n",
        "\n",
        "        self.decoder_hidden = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "        self.decoder_cell = Variable(memory.data.new(\n",
        "            B, self.decoder_rnn_dim).zero_())\n",
        "\n",
        "        self.attention_weights = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_weights_cum = Variable(memory.data.new(\n",
        "            B, MAX_TIME).zero_())\n",
        "        self.attention_context = Variable(memory.data.new(\n",
        "            B, self.encoder_embedding_dim).zero_())\n",
        "\n",
        "        self.memory = memory\n",
        "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
        "        self.mask = mask\n",
        "\n",
        "    def parse_decoder_inputs(self, decoder_inputs):\n",
        "        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
        "        decoder_inputs = decoder_inputs.view(\n",
        "            decoder_inputs.size(0),\n",
        "            int(decoder_inputs.size(1) / self.n_frames_per_step), -1)\n",
        "        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
        "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
        "        return decoder_inputs\n",
        "\n",
        "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        alignments = torch.stack(alignments).transpose(0, 1)\n",
        "        # (T_out, B) -> (B, T_out)\n",
        "        gate_outputs = torch.stack(gate_outputs)\n",
        "        if len(gate_outputs.size()) > 1:\n",
        "            gate_outputs = gate_outputs.transpose(0, 1)\n",
        "        else:\n",
        "            gate_outputs = gate_outputs[None]\n",
        "        gate_outputs = gate_outputs.contiguous()\n",
        "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
        "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
        "        # decouple frames per step\n",
        "        mel_outputs = mel_outputs.view(\n",
        "            mel_outputs.size(0), -1, self.n_mel_channels)\n",
        "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
        "        mel_outputs = mel_outputs.transpose(1, 2)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def decode(self, decoder_input, attention_weights=None):\n",
        "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
        "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
        "            cell_input, (self.attention_hidden, self.attention_cell))\n",
        "        self.attention_hidden = F.dropout(\n",
        "            self.attention_hidden, self.p_attention_dropout, self.training)\n",
        "        self.attention_cell = F.dropout(\n",
        "            self.attention_cell, self.p_attention_dropout, self.training)\n",
        "\n",
        "        attention_weights_cat = torch.cat(\n",
        "            (self.attention_weights.unsqueeze(1),\n",
        "             self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
        "        self.attention_context, self.attention_weights = self.attention_layer(\n",
        "            self.attention_hidden, self.memory, self.processed_memory,\n",
        "            attention_weights_cat, self.mask, attention_weights)\n",
        "\n",
        "        self.attention_weights_cum += self.attention_weights\n",
        "        decoder_input = torch.cat(\n",
        "            (self.attention_hidden, self.attention_context), -1)\n",
        "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
        "            decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
        "        self.decoder_hidden = F.dropout(\n",
        "            self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
        "        self.decoder_cell = F.dropout(\n",
        "            self.decoder_cell, self.p_decoder_dropout, self.training)\n",
        "\n",
        "        decoder_hidden_attention_context = torch.cat(\n",
        "            (self.decoder_hidden, self.attention_context), dim=1)\n",
        "\n",
        "        decoder_output = self.linear_projection(\n",
        "            decoder_hidden_attention_context)\n",
        "\n",
        "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
        "        return decoder_output, gate_prediction, self.attention_weights\n",
        "\n",
        "    def forward(self, memory, decoder_inputs, memory_lengths, f0s):\n",
        "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
        "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
        "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
        "        decoder_inputs = self.prenet(decoder_inputs)\n",
        "\n",
        "        # audio features\n",
        "        f0_dummy = self.get_end_f0(f0s)\n",
        "        f0s = torch.cat((f0s, f0_dummy), dim=2)\n",
        "        f0s = F.relu(self.prenet_f0(f0s))\n",
        "        f0s = f0s.permute(2, 0, 1)\n",
        "\n",
        "        self.initialize_decoder_states(\n",
        "            memory, mask=~get_mask_from_lengths(memory_lengths))\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
        "            if len(mel_outputs) == 0 or np.random.uniform(0.0, 1.0) <= self.p_teacher_forcing:\n",
        "                decoder_input = torch.cat((decoder_inputs[len(mel_outputs)],\n",
        "                                           f0s[len(mel_outputs)]), dim=1)\n",
        "            else:\n",
        "                decoder_input = torch.cat((self.prenet(mel_outputs[-1]),\n",
        "                                           f0s[len(mel_outputs)]), dim=1)\n",
        "            mel_output, gate_output, attention_weights = self.decode(\n",
        "                decoder_input)\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output.squeeze()]\n",
        "            alignments += [attention_weights]\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def inference(self, memory, f0s):\n",
        "        decoder_input = self.get_go_frame(memory)\n",
        "\n",
        "        self.initialize_decoder_states(memory, mask=None)\n",
        "        f0_dummy = self.get_end_f0(f0s)\n",
        "        f0s = torch.cat((f0s, f0_dummy), dim=2)\n",
        "        f0s = F.relu(self.prenet_f0(f0s))\n",
        "        f0s = f0s.permute(2, 0, 1)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        while True:\n",
        "            if len(mel_outputs) < len(f0s):\n",
        "                f0 = f0s[len(mel_outputs)]\n",
        "            else:\n",
        "                f0 = f0s[-1] * 0\n",
        "\n",
        "            decoder_input = torch.cat((self.prenet(decoder_input), f0), dim=1)\n",
        "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
        "\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output]\n",
        "            alignments += [alignment]\n",
        "\n",
        "            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
        "                break\n",
        "            elif len(mel_outputs) == self.max_decoder_steps:\n",
        "                print(\"Warning! Reached max decoder steps\")\n",
        "                break\n",
        "\n",
        "            decoder_input = mel_output\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments\n",
        "\n",
        "    def inference_noattention(self, memory, f0s, attention_map):\n",
        "        decoder_input = self.get_go_frame(memory)\n",
        "\n",
        "        self.initialize_decoder_states(memory, mask=None)\n",
        "        f0_dummy = self.get_end_f0(f0s)\n",
        "        f0s = torch.cat((f0s, f0_dummy), dim=2)\n",
        "        f0s = F.relu(self.prenet_f0(f0s))\n",
        "        f0s = f0s.permute(2, 0, 1)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = [], [], []\n",
        "        for i in range(len(attention_map)):\n",
        "            f0 = f0s[i]\n",
        "            attention = attention_map[i]\n",
        "            decoder_input = torch.cat((self.prenet(decoder_input), f0), dim=1)\n",
        "            mel_output, gate_output, alignment = self.decode(decoder_input, attention)\n",
        "\n",
        "            mel_outputs += [mel_output.squeeze(1)]\n",
        "            gate_outputs += [gate_output]\n",
        "            alignments += [alignment]\n",
        "\n",
        "            decoder_input = mel_output\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
        "            mel_outputs, gate_outputs, alignments)\n",
        "\n",
        "        return mel_outputs, gate_outputs, alignments"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_aRMfT-OLtk"
      },
      "source": [
        "### Style token layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWMtCBkjcIYR"
      },
      "source": [
        "class ReferenceEncoder(nn.Module):\n",
        "    def __init__(self, hp):\n",
        "\n",
        "        super().__init__()\n",
        "        K = len(hp.ref_enc_filters)\n",
        "        filters = [1] + hp.ref_enc_filters\n",
        "\n",
        "        convs = [nn.Conv2d(in_channels=filters[i],\n",
        "                           out_channels=filters[i + 1],\n",
        "                           kernel_size=(3, 3),\n",
        "                           stride=(2, 2),\n",
        "                           padding=(1, 1)) for i in range(K)]\n",
        "        self.convs = nn.ModuleList(convs)\n",
        "        self.bns = nn.ModuleList(\n",
        "            [nn.BatchNorm2d(num_features=hp.ref_enc_filters[i])\n",
        "             for i in range(K)])\n",
        "\n",
        "        out_channels = self.calculate_channels(hp.n_mel_channels, 3, 2, 1, K)\n",
        "        self.gru = nn.GRU(input_size=hp.ref_enc_filters[-1] * out_channels,\n",
        "                          hidden_size=hp.ref_enc_gru_size,\n",
        "                          batch_first=True)\n",
        "        self.n_mel_channels = hp.n_mel_channels\n",
        "        self.ref_enc_gru_size = hp.ref_enc_gru_size\n",
        "\n",
        "    def forward(self, inputs, input_lengths=None):\n",
        "        out = inputs.view(inputs.size(0), 1, -1, self.n_mel_channels)\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            out = conv(out)\n",
        "            out = bn(out)\n",
        "            out = F.relu(out)\n",
        "\n",
        "        out = out.transpose(1, 2)  # [N, Ty//2^K, 128, n_mels//2^K]\n",
        "        N, T = out.size(0), out.size(1)\n",
        "        out = out.contiguous().view(N, T, -1)  # [N, Ty//2^K, 128*n_mels//2^K]\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            input_lengths = torch.ceil(input_lengths.float() / 2 ** len(self.convs))\n",
        "            input_lengths = input_lengths.cpu().numpy().astype(int)\n",
        "            out = nn.utils.rnn.pack_padded_sequence(\n",
        "                out, input_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        self.gru.flatten_parameters()\n",
        "        _, out = self.gru(out)\n",
        "        return out.squeeze(0)\n",
        "\n",
        "    def calculate_channels(self, L, kernel_size, stride, pad, n_convs):\n",
        "        for _ in range(n_convs):\n",
        "            L = (L - kernel_size + 2 * pad) // stride + 1\n",
        "        return L\n",
        "\n",
        "\n",
        "class STL(nn.Module):\n",
        "    def __init__(self, hp):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Parameter(torch.FloatTensor(hp.token_num, hp.token_embedding_size // hp.num_heads))\n",
        "        d_q = hp.ref_enc_gru_size\n",
        "        d_k = hp.token_embedding_size // hp.num_heads\n",
        "        self.attention = MultiHeadAttention(\n",
        "            query_dim=d_q, key_dim=d_k, num_units=hp.token_embedding_size,\n",
        "            num_heads=hp.num_heads)\n",
        "\n",
        "        init.normal_(self.embed, mean=0, std=0.5)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        N = inputs.size(0)\n",
        "        query = inputs.unsqueeze(1)\n",
        "        keys = torch.tanh(self.embed).unsqueeze(0).expand(N, -1,\n",
        "                                                          -1)  # [N, token_num, token_embedding_size // num_heads]\n",
        "        style_embed = self.attention(query, keys)\n",
        "\n",
        "        return style_embed\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, query_dim, key_dim, num_units, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_units = num_units\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "\n",
        "        self.W_query = nn.Linear(in_features=query_dim, out_features=num_units, bias=False)\n",
        "        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n",
        "        self.W_value = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n",
        "\n",
        "    def forward(self, query, key):\n",
        "        querys = self.W_query(query)  # [N, T_q, num_units]\n",
        "        keys = self.W_key(key)  # [N, T_k, num_units]\n",
        "        values = self.W_value(key)\n",
        "\n",
        "        split_size = self.num_units // self.num_heads\n",
        "        querys = torch.stack(torch.split(querys, split_size, dim=2), dim=0)  # [h, N, T_q, num_units/h]\n",
        "        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n",
        "        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)  # [h, N, T_k, num_units/h]\n",
        "\n",
        "        # score = softmax(QK^T / (d_k ** 0.5))\n",
        "        scores = torch.matmul(querys, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n",
        "        scores = scores / (self.key_dim ** 0.5)\n",
        "        scores = F.softmax(scores, dim=3)\n",
        "\n",
        "        # out = score * V\n",
        "        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n",
        "        out = torch.cat(torch.split(out, 1, dim=0), dim=3).squeeze(0)  # [N, T_q, num_units]\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class GST(nn.Module):\n",
        "    def __init__(self, hp):\n",
        "        super().__init__()\n",
        "        self.encoder = ReferenceEncoder(hp)\n",
        "        self.stl = STL(hp)\n",
        "\n",
        "    def forward(self, inputs, input_lengths=None):\n",
        "        enc_out = self.encoder(inputs, input_lengths=input_lengths)\n",
        "        style_embed = self.stl(enc_out)\n",
        "\n",
        "        return style_embed"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDbB18HYOSgJ"
      },
      "source": [
        "### Seq2Seq Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bj1J3DZcPoJ"
      },
      "source": [
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(Tacotron2, self).__init__()\n",
        "        self.mask_padding = hparams.mask_padding\n",
        "        self.fp16_run = hparams.fp16_run\n",
        "        self.n_mel_channels = hparams.n_mel_channels\n",
        "        self.n_frames_per_step = hparams.n_frames_per_step\n",
        "        self.embedding = nn.Embedding(\n",
        "            hparams.n_symbols, hparams.symbols_embedding_dim)\n",
        "        std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))\n",
        "        val = sqrt(3.0) * std  # uniform bounds for std\n",
        "        self.embedding.weight.data.uniform_(-val, val)\n",
        "        self.encoder = Encoder(hparams)\n",
        "        self.decoder = Decoder(hparams)\n",
        "        self.postnet = Postnet(hparams)\n",
        "        if hparams.with_gst:\n",
        "            self.gst = GST(hparams)\n",
        "        self.speaker_embedding = nn.Embedding(\n",
        "            hparams.n_speakers, hparams.speaker_embedding_dim)\n",
        "\n",
        "    def parse_batch(self, batch):\n",
        "        text_padded, input_lengths, mel_padded, gate_padded, \\\n",
        "        output_lengths, speaker_ids, f0_padded = batch\n",
        "        text_padded = to_gpu(text_padded).long()\n",
        "        input_lengths = to_gpu(input_lengths).long()\n",
        "        max_len = torch.max(input_lengths.data).item()\n",
        "        mel_padded = to_gpu(mel_padded).float()\n",
        "        gate_padded = to_gpu(gate_padded).float()\n",
        "        output_lengths = to_gpu(output_lengths).long()\n",
        "        speaker_ids = to_gpu(speaker_ids.data).long()\n",
        "        f0_padded = to_gpu(f0_padded).float()\n",
        "        return ((text_padded, input_lengths, mel_padded, max_len,\n",
        "                 output_lengths, speaker_ids, f0_padded),\n",
        "                (mel_padded, gate_padded))\n",
        "\n",
        "    def parse_output(self, outputs, output_lengths=None):\n",
        "        if self.mask_padding and output_lengths is not None:\n",
        "            mask = ~get_mask_from_lengths(output_lengths)\n",
        "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
        "            mask = mask.permute(1, 0, 2)\n",
        "\n",
        "            outputs[0].data.masked_fill_(mask, 0.0)\n",
        "            outputs[1].data.masked_fill_(mask, 0.0)\n",
        "            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs, input_lengths, targets, max_len, \\\n",
        "        output_lengths, speaker_ids, f0s = inputs\n",
        "        input_lengths, output_lengths = input_lengths.data, output_lengths.data\n",
        "\n",
        "        embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
        "        embedded_text = self.encoder(embedded_inputs, input_lengths)\n",
        "        embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
        "        embedded_gst = self.gst(targets, output_lengths)\n",
        "        embedded_gst = embedded_gst.repeat(1, embedded_text.size(1), 1)\n",
        "        embedded_speakers = embedded_speakers.repeat(1, embedded_text.size(1), 1)\n",
        "\n",
        "        encoder_outputs = torch.cat(\n",
        "            (embedded_text, embedded_gst, embedded_speakers), dim=2)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
        "            encoder_outputs, targets, memory_lengths=input_lengths, f0s=f0s)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        return self.parse_output(\n",
        "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
        "            output_lengths)\n",
        "\n",
        "    def inference(self, inputs):\n",
        "        text, style_input, speaker_ids, f0s = inputs\n",
        "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
        "        embedded_text = self.encoder.inference(embedded_inputs)\n",
        "        embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
        "        if hasattr(self, 'gst'):\n",
        "            if isinstance(style_input, int):\n",
        "                query = torch.zeros(1, 1, self.gst.encoder.ref_enc_gru_size).cuda()\n",
        "                GST = torch.tanh(self.gst.stl.embed)\n",
        "                key = GST[style_input].unsqueeze(0).expand(1, -1, -1)\n",
        "                embedded_gst = self.gst.stl.attention(query, key)\n",
        "            else:\n",
        "                embedded_gst = self.gst(style_input)\n",
        "\n",
        "        embedded_speakers = embedded_speakers.repeat(1, embedded_text.size(1), 1)\n",
        "        if hasattr(self, 'gst'):\n",
        "            embedded_gst = embedded_gst.repeat(1, embedded_text.size(1), 1)\n",
        "            encoder_outputs = torch.cat(\n",
        "                (embedded_text, embedded_gst, embedded_speakers), dim=2)\n",
        "        else:\n",
        "            encoder_outputs = torch.cat(\n",
        "                (embedded_text, embedded_speakers), dim=2)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
        "            encoder_outputs, f0s)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        return self.parse_output(\n",
        "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
        "\n",
        "    def inference_noattention(self, inputs):\n",
        "        text, style_input, speaker_ids, f0s, attention_map = inputs\n",
        "        embedded_inputs = self.embedding(text).transpose(1, 2)\n",
        "        embedded_text = self.encoder.inference(embedded_inputs)\n",
        "        embedded_speakers = self.speaker_embedding(speaker_ids)[:, None]\n",
        "        if hasattr(self, 'gst'):\n",
        "            if isinstance(style_input, int):\n",
        "                query = torch.zeros(1, 1, self.gst.encoder.ref_enc_gru_size).cuda()\n",
        "                GST = torch.tanh(self.gst.stl.embed)\n",
        "                key = GST[style_input].unsqueeze(0).expand(1, -1, -1)\n",
        "                embedded_gst = self.gst.stl.attention(query, key)\n",
        "            else:\n",
        "                embedded_gst = self.gst(style_input)\n",
        "\n",
        "        embedded_speakers = embedded_speakers.repeat(1, embedded_text.size(1), 1)\n",
        "        if hasattr(self, 'gst'):\n",
        "            embedded_gst = embedded_gst.repeat(1, embedded_text.size(1), 1)\n",
        "            encoder_outputs = torch.cat(\n",
        "                (embedded_text, embedded_gst, embedded_speakers), dim=2)\n",
        "        else:\n",
        "            encoder_outputs = torch.cat(\n",
        "                (embedded_text, embedded_speakers), dim=2)\n",
        "\n",
        "        mel_outputs, gate_outputs, alignments = self.decoder.inference_noattention(\n",
        "            encoder_outputs, f0s, attention_map)\n",
        "\n",
        "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
        "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
        "\n",
        "        return self.parse_output(\n",
        "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CQSBff0OtHj"
      },
      "source": [
        "### Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C88_bdVOvJp"
      },
      "source": [
        "class Tacotron2Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Tacotron2Loss, self).__init__()\n",
        "\n",
        "    def forward(self, model_output, targets):\n",
        "        mel_target, gate_target = targets[0], targets[1]\n",
        "        mel_target.requires_grad = False\n",
        "        gate_target.requires_grad = False\n",
        "        gate_target = gate_target.view(-1, 1)\n",
        "\n",
        "        mel_out, mel_out_postnet, gate_out, _ = model_output\n",
        "        gate_out = gate_out.view(-1, 1)\n",
        "        mel_loss = nn.MSELoss()(mel_out, mel_target) + \\\n",
        "            nn.MSELoss()(mel_out_postnet, mel_target)\n",
        "        gate_loss = nn.BCEWithLogitsLoss()(gate_out, gate_target)\n",
        "        return mel_loss + gate_loss\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw0dUpjTS_Vy"
      },
      "source": [
        "torch.manual_seed(hparams.seed)\n",
        "torch.cuda.manual_seed(hparams.seed)\n",
        "criterion = Tacotron2Loss()\n",
        "\n",
        "model = Tacotron2(hparams).cuda()\n",
        "learning_rate = hparams.learning_rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                             weight_decay=hparams.weight_decay)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJVcehmIcxuF"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTJ61zEYc8gF",
        "outputId": "78ffeb88-dc45-4237-ce90-3e3d37578c81"
      },
      "source": [
        "train_loader, valset, collate_fn, train_sampler = prepare_dataloaders(hparams)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "speaker id  ['1' '2' '3']\n",
            "seected speaker id  {1: 0, 2: 1, 3: 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "rwXAnQbscycn",
        "outputId": "65d72ccb-89b2-42a0-919d-c9200cc91037"
      },
      "source": [
        "model.train()\n",
        "is_overflow = False\n",
        "iteration = 0\n",
        "# ================ MAIN TRAINNIG LOOP! ===================\n",
        "for epoch in range(0, hparams.epochs):\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        start = time.perf_counter()\n",
        "\n",
        "        model.zero_grad()\n",
        "        x, y = model.parse_batch(batch)\n",
        "        print(len(x))\n",
        "        y_pred = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        reduced_loss = loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "        optimizer.step()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "7\n",
            "here\n",
            "7\n",
            "here\n",
            "7\n",
            "here\n",
            "7\n",
            "here\n",
            "7\n",
            "here\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-750d864ab9a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-de02bd7b50ec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         mel_outputs, gate_outputs, alignments = self.decoder(\n\u001b[0;32m---> 62\u001b[0;31m             encoder_outputs, targets, memory_lengths=input_lengths, f0s=f0s)\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmel_outputs_postnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-b2cc01b3a99d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, memory, decoder_inputs, memory_lengths, f0s)\u001b[0m\n\u001b[1;32m    170\u001b[0m                                            f0s[len(mel_outputs)]), dim=1)\n\u001b[1;32m    171\u001b[0m             mel_output, gate_output, attention_weights = self.decode(\n\u001b[0;32m--> 172\u001b[0;31m                 decoder_input)\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mmel_outputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mgate_outputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgate_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-b2cc01b3a99d>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, decoder_input, attention_weights)\u001b[0m\n\u001b[1;32m    131\u001b[0m             (self.attention_hidden, self.attention_context), -1)\n\u001b[1;32m    132\u001b[0m         self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n\u001b[0;32m--> 133\u001b[0;31m             decoder_input, (self.decoder_hidden, self.decoder_cell))\n\u001b[0m\u001b[1;32m    134\u001b[0m         self.decoder_hidden = F.dropout(\n\u001b[1;32m    135\u001b[0m             self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m         )\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_X9dTcQyKDo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}